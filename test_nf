#!/usr/bin/env nextflow


params.reads = 'data/RochePanCancer/20M02656/*_R{1,2}_001.fastq.gz'
params.reference_genome = 'resources/human_g1k_v37.fasta'
params.capturebed = 'config/RochePanCancer/180702_HG19_PanCancer_EZ_capture_targets.bed'
params.primarybed = 'config/RochePanCancer/180702_HG19_PanCancer_EZ_primary_targets.bed'
params.seqId = '200221_NB551319_0071_AH2HCJAFX2'
params.chromosomes = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "X", "Y", "MT"]
params.read1Adapter = 'AGATCGGAAGAGC'
params.read2Adapter = 'AGATCGGAAGAGC' 
params.worklistId = '20-958'
params.sex = '2'
params.referral = '2'
params.panel = 'RochePanCancer'
params.pipelinename = 'SomaticEnrichment'
params.expectedInsertSize = '150'
params.sampleId = '20M02656'
params.sequencing_centre = 'AWMGL'
params.refdict = 'resources/human_g1k_v37.dict'
params.refidx = 'resources/human_g1k_v37.fasta.fai'

reference_genome = file(params.reference_genome)
capture_bed = file(params.capturebed)
refdict= file(params.refdict)
refidx= file(params.refidx)



Channel
  .fromFilePairs(params.reads, flat: true) 
  .set { reads_ch }


chromosomes_ch = Channel
    .from( params.chromosomes)

/*reads_ch.subscribe { println "$it"}*/



process removeadapters {
	input:
    set val(id), file(read1), file(read2) from reads_ch 

    output:
    set val(id), file("${params.seqId}_${params.sampleId}_${laneId}_R1.fastq") into trimmed_r1
    set val(id), file("${params.seqId}_${params.sampleId}_${laneId}_R2.fastq") into trimmed_r2

    script:
    laneId = read1.name.toString().tokenize('_').get(2)


    """
    cutadapt.sh \
        ${params.seqId} \
        ${params.sampleId} \
        ${laneId} \
        ${read1} \
        ${read2}\
        ${params.read1Adapter} \
        ${params.read2Adapter}
    """
}

trimmed_r1.into{
	trimmed_r1_fqc
	trimmed_r1_ubam
	trimmed_r1_bam
}

trimmed_r2.into{
	trimmed_r2_fqc
	trimmed_r2_ubam
	trimmed_r2_bam
}



process collate_fastqcmetrics {

	 publishDir "results/fastqc/"
	
	input:
	set val(id), file(r1_fastq) from trimmed_r1_fqc
	set val(id), file(r2_fastq) from trimmed_r2_fqc

	"""
	
	fastqc.sh ${r1_fastq} ${r2_fastq}

	"""
}



process align_bamfiles {

	input:
	set val(id), file(read1) from trimmed_r1_bam
	set val(id), file(read2) from trimmed_r2_bam

	output:
    file "${params.seqId}_${params.sampleId}_${laneId}.bam" into aligned_bam
    file "${params.seqId}_${params.sampleId}_${laneId}.bam.bai" into aligned_bam_indexes
    script:
    laneId = read1.name.toString().tokenize('_').get(5)
    
    """
	bwa mem \
    -v 1 \
    -t 12 \
    -M \
    -R '@RG\\tID:${params.seqId}.${laneId}\\tCN:${params.sequencing_centre}\\tSM:${params.sampleId}\\tLB:${params.seqId}\\tPL:ILLUMINA' \
    $reference_genome \
    $read1 \
    $read2 | samtools view -Sb - | \
    samtools sort -T . -O bam > "${params.seqId}_${params.sampleId}_${laneId}.bam" 
    samtools index "${params.seqId}_${params.sampleId}_${laneId}.bam"
	"""
}



/*
process align_bamfiles2 {

	input:
	set val(id), file(read1) from trimmed_r1_ubam
	set val(id), file(read2) from trimmed_r2_ubam

	output:
    file "${params.seqId}_${params.sampleId}_${laneId}.bam" into aligned_bam2
    file "${params.seqId}_${params.sampleId}_${laneId}.bam.bai" into aligned_bam_indexes2
    script:
    laneId = read1.name.toString().tokenize('_').get(5)
    
    """
	bwa2.sh \
    ${params.seqId} \
    ${laneId} \
    ${params.sequencing_centre} \
    ${params.sampleId} \
    $reference_genome \
    $read1 \
    $read2 
	"""
}


*/





/*
Channel 
    aligned_bam.map { file ->
        def key = file.name.toString().tokenize('_').get(0)
        return tuple(key, file)
    }
    .groupTuple()
    .set{grouped_bam_ch}




process merge_and_removeduplicates {

    input:
    set key, file(samples) from  grouped_bam_ch


    script:

    """

    echo --batch $key --input $samples

    """
}

*/


/*

        publishDir "results/final_bams/"



    output:
    file("${params.seqId}_${key}_mdup.bam") into mdup_bam
    file("${params.seqId}_${key}_mdup.bam.bai") into mdup_bam_index
    file("${params.seqId}_${key}_MarkDuplicateMetrics.txt")
    """
    picard MarkDuplicates \
    ${bams.collect { "I=$it" }.join()} \
    O=${params.seqId}_${key}_mdup.bam \
    METRICS_FILE=${params.seqId}_${key}_MarkDuplicateMetrics.txt \
    CREATE_INDEX=true \
    VALIDATION_STRINGENCY=SILENT \
    MAX_RECORDS_IN_RAM=2000000 \
    TMP_DIR= . \
    QUIET=true \
    VERBOSITY=ERROR
    """
}
*/



process merge_bams_and_remove_duplicates{

        publishDir "results/final_bams/"

    input:
    set val(key), file(bams) from aligned_bam.map { file ->
    def key = file.name.toString().tokenize('_').get(0)
    return tuple(key, file)
    }
    .groupTuple()

    set val(key2), file(bam_indexes) from aligned_bam_indexes.map { file ->
    def key = file.name.toString().tokenize('_').get(0)
    return tuple(key, file)
    }
    .groupTuple()

    output:
    file("${key}_markduplicates.bam") into mark_duplicates_bam_channel
    file("${key}_markduplicates.bai") into mark_duplicates_bam_index_channel
    file("${key}_markduplicate_metrics.txt")

    """
    picard MarkDuplicates \
    ${bams.collect { "I=$it " }.join()} \
    O=${key}_markduplicates.bam \
    METRICS_FILE=${key}_markduplicate_metrics.txt \
    CREATE_INDEX=true \
    VALIDATION_STRINGENCY=SILENT \
    MAX_RECORDS_IN_RAM=2000000 \
    TMP_DIR= . 
    """
}

