#!/usr/bin/env nextflow


params.reads = 'data/RochePanCancer/20M02656/*_R{1,2}_001.fastq.gz'
params.reference_genome = 'resources/human_g1k_v37.fasta'
params.refidx = 'resources/human_g1k_v37.fasta.fai'
params.refdict = 'resources/human_g1k_v37.dict'
params.bedtools_genome = 'resources/human.hg19.genome'
params.capturebed = 'config/RochePanCancer/180702_HG19_PanCancer_EZ_capture_targets.bed'
params.primarybed = 'config/RochePanCancer/180702_HG19_PanCancer_EZ_primary_targets.bed'
params.seqId = '200221_NB551319_0071_AH2HCJAFX2'
params.chromosomes = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "X", "Y", "MT"]
params.read1Adapter = 'AGATCGGAAGAGC'
params.read2Adapter = 'AGATCGGAAGAGC' 
params.minimiumCoverage="250, 135"
params.expectedInsertSize = '150'
params.padding='250'
params.minBQS='10'
params.minMQS='20'


params.worklistId = '20-958'
params.sex = '2'
params.referral = '2'
params.panel = 'RochePanCancer'
params.pipelinename = 'SomaticEnrichment'
params.pipelineversion = '1.2.0'
params.sampleId = '20M02656'
params.sequencing_centre = 'AWMGL'


reference_genome = file(params.reference_genome)
bedtools_genome = file(params.bedtools_genome)
refidx = file(params.refidx)
refdict = file(params.refdict)
capture_bed = file(params.capturebed)
primary_bed = file(params.primarybed)





Channel
  .fromFilePairs(params.reads, flat: true) 
  .set { reads_ch }


chromosomes_ch = Channel
    .from( params.chromosomes)

/*reads_ch.subscribe { println "$it"}*/



process removeadapters {
	input:
    set val(id), file(read1), file(read2) from reads_ch 

    output:
    set val(id), file("${params.seqId}_${params.sampleId}_${laneId}_R1.fastq") into trimmed_r1
    set val(id), file("${params.seqId}_${params.sampleId}_${laneId}_R2.fastq") into trimmed_r2

    script:
    laneId = read1.name.toString().tokenize('_').get(2)


    """
    cutadapt.sh \
        ${params.seqId} \
        ${params.sampleId} \
        ${laneId} \
        ${read1} \
        ${read2}\
        ${params.read1Adapter} \
        ${params.read2Adapter}
    """
}

trimmed_r1.into{
	trimmed_r1_fqc
	trimmed_r1_ubam
	trimmed_r1_bam
}

trimmed_r2.into{
	trimmed_r2_fqc
	trimmed_r2_ubam
	trimmed_r2_bam
}



process collate_fastqcmetrics {

	 publishDir "results/fastqc/"
	
	input:
	set val(id), file(r1_fastq) from trimmed_r1_fqc
	set val(id), file(r2_fastq) from trimmed_r2_fqc

	"""
	
	fastqc.sh ${r1_fastq} ${r2_fastq}

	"""
}



process align_bamfiles {

	input:
	set val(id), file(read1) from trimmed_r1_bam
	set val(id), file(read2) from trimmed_r2_bam

	output:
    file "${params.seqId}_${params.sampleId}_${laneId}.bam" into aligned_bam
    file "${params.seqId}_${params.sampleId}_${laneId}.bam.bai" into aligned_bam_indexes
    script:
    laneId = read1.name.toString().tokenize('_').get(5)
    
    """
	bwa mem \
    -v 1 \
    -t 12 \
    -M \
    -R '@RG\\tID:${params.seqId}.${laneId}\\tCN:${params.sequencing_centre}\\tSM:${params.sampleId}\\tLB:${params.seqId}\\tPL:ILLUMINA' \
    $reference_genome \
    $read1 \
    $read2 | samtools view -Sb - | \
    samtools sort -T . -O bam > "${params.seqId}_${params.sampleId}_${laneId}.bam" 
    samtools index "${params.seqId}_${params.sampleId}_${laneId}.bam"
	"""
}



process merge_bams_and_remove_duplicates{

        publishDir "results/final_bams/"

    input:
    set val(key), file(bams) from aligned_bam.map { file ->
    def key = file.name.toString().tokenize('_').get(4)
    return tuple(key, file)
    }
    .groupTuple()

    set val(key2), file(bam_indexes) from aligned_bam_indexes.map { file ->
    def key = file.name.toString().tokenize('_').get(4)
    return tuple(key, file)
    }
    .groupTuple()

    output:
    file("${params.seqId}_${key}.bam") into mark_duplicates_bam_channel
    file("${params.seqId}_${key}.bai") into mark_duplicates_bam_index_channel
    file("${params.seqId}_${key}_markduplicate_metrics.txt")

    """
    picard MarkDuplicates \
    ${bams.collect { "I=$it " }.join()} \
    O=${params.seqId}_${key}.bam \
    METRICS_FILE=${params.seqId}_${key}_markduplicate_metrics.txt \
    CREATE_INDEX=true \
    VALIDATION_STRINGENCY=SILENT \
    MAX_RECORDS_IN_RAM=2000000 \
    TMP_DIR= . 
    """
}


mark_duplicates_bam_channel.into{
    bam_QC
    bam_cov
}


mark_duplicates_bam_index_channel.into{
    bai_QC
    bai_cov
}




process post_alignment_QC {

    input: 

    file(bam) from bam_QC
    file(bai) from bai_QC

    output: 

    "${params.seqId}_${params.sampleId}_AlignmentSummaryMetrics.txt"
    "${params.seqId}_${params.sampleId}_InsertMetrics.txt"
    "${params.seqId}_${params.sampleId}_HsMetrics.txt"

    script:
    sampleId = bam.name.toString().tokenize('_').get(4)

    """
    post_alignment_qc.sh \
    ${params.seqId} \
    ${params.sampleId} \
    ${params.panel} \
    ${params.minimiumCoverage} \
    $capture_bed \
    $primary_bed \
    ${params.padding} \
    ${params.minBQS} \
    ${params.minMQS} \
    $reference_genome \
    $refdict
    """
}

/*"${params.panel}_capture.interval_list"
"${params.panel}_primary.interval_list" */



